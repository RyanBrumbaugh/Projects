{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNa7KBir1fXt8G4CuV0Ay1u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyanBrumbaugh/Projects/blob/main/Improvement_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wRjHSRUEMHq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def sw_removal(comment, stop_words):\n",
        "    text_tokens = word_tokenize(comment)\n",
        "    tokens_without_sw = [word for word in text_tokens if word.lower() not in stop_words]\n",
        "    return \" \".join(tokens_without_sw)\n",
        "\n",
        "def process_data(data, column_name, stop_words):\n",
        "    data['Last Updated'] = pd.to_datetime(data['Last Updated'], errors='coerce')\n",
        "    data = data[data['Last Updated'].dt.year == 2023]\n",
        "\n",
        "    data['Compensation Grade'] = data['Compensation Grade'].replace({'J060': '35', 'J070': '45', 'J080': '45', 'J090': '45',\n",
        "                                                                     'J100': '55', 'J110': '60', 'J120': '65', 'J130': '70',\n",
        "                                                                     'J140': '70'})\n",
        "    data['Compensation Grade'] = pd.to_numeric(data['Compensation Grade'], errors='coerce')\n",
        "    data = data[data['Compensation Grade'] >= 60]\n",
        "\n",
        "    processed_data = data.copy()\n",
        "    columns_to_drop = ['Retention', 'Strengths', 'Potential Assessment Notes',\n",
        "                       'Current Year -1 Review Rating', 'Current Year -2 Review Rating', 'Hire Date', 'Company',\n",
        "                       'Location', 'Location Address - Country', 'Job Title', 'Compensation Grade', 'Segment',\n",
        "                       'Cost Center', 'Functional Area Code', 'Functional Area Name', 'Functional Department',\n",
        "                       'Reporting Unit', 'Current Year -1 Completed Review', 'Employee ID', 'Manager - Level 01',\n",
        "                       'Manager Level 01', 'Manager Level 02', 'Manager Level 03', 'Manager Level 04', 'Manager Level 05',\n",
        "                       'Manager Level 06', 'Manager Level 07', 'Manager Level 08', 'Manager Level 09',\n",
        "                       'Supervisory Organization', 'Last Updated', 'Current Year -2 Completed Review']\n",
        "    processed_data = processed_data.drop(columns_to_drop, axis=1).dropna()\n",
        "\n",
        "    stop_words.update(['viatris', 'company'])\n",
        "    processed_data[column_name] = processed_data[column_name].apply(lambda x: sw_removal(x, stop_words))\n",
        "\n",
        "    vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))\n",
        "    column_values = processed_data[column_name].tolist()\n",
        "    X = vectorizer.fit_transform(column_values)\n",
        "\n",
        "    return processed_data, X, vectorizer\n",
        "\n",
        "def predict_categories(X, category_descriptions):\n",
        "    Y = vectorizer.transform(category_descriptions)\n",
        "    similarity_scores = cosine_similarity(X, Y)\n",
        "    predicted_categories = [list(categories.keys())[score.argmax()] for score in similarity_scores]\n",
        "    return predicted_categories\n",
        "\n",
        "# Read data\n",
        "input_file_path = 'Ryans Report.csv'\n",
        "data = pd.read_csv(input_file_path)\n",
        "\n",
        "# Process strengths data\n",
        "strengths_data, X_strengths, vectorizer = process_data(data, 'Strengths', stop_words)\n",
        "\n",
        "# Process areas of opportunity data\n",
        "areas_data, X_areas, _ = process_data(data, 'Areas Of Opportunity', stop_words)\n",
        "\n",
        "# Predict categories for strengths and areas of opportunity\n",
        "strengths_predicted_categories = predict_categories(X_strengths, category_descriptions)\n",
        "areas_predicted_categories = predict_categories(X_areas, category_descriptions)\n",
        "\n",
        "# Add predicted categories to the dataframes\n",
        "strengths_data['Strengths Predicted Category'] = strengths_predicted_categories\n",
        "areas_data['Areas Of Opportunity Predicted Category'] = areas_predicted_categories\n",
        "\n",
        "# Merge dataframes\n",
        "merged_data = strengths_data.merge(areas_data[['Worker', 'Areas Of Opportunity Predicted Category']], on='Worker', how='left')\n",
        "final_merged_data = data.merge(merged_data[['Worker', 'Strengths Predicted Category', 'Areas Of Opportunity Predicted Category']], on='Worker', how='left')\n",
        "\n",
        "# Export to Excel\n",
        "output_file_path = r'\\\\file_path'\n",
        "final_merged_data.to_excel(output_file_path, index=False, encoding='utf-8')\n",
        "\n",
        "print(\"DataFrame exported to Excel with UTF-8 encoding successfully.\")\n"
      ]
    }
  ]
}