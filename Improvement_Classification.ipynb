{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzGf+oZgJk1rGhLtSSfVT2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyanBrumbaugh/Projects/blob/main/Improvement_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wRjHSRUEMHq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def sw_removal(comment, stop_words):\n",
        "    text_tokens = word_tokenize(comment)\n",
        "    tokens_without_sw = [word for word in text_tokens if word.lower() not in stop_words]\n",
        "    return \" \".join(tokens_without_sw)\n",
        "\n",
        "def process_data(data, column_name, stop_words):\n",
        "    data['Last Updated'] = pd.to_datetime(data['Last Updated'], errors='coerce')\n",
        "    data = data[data['Last Updated'].dt.year == 2023]\n",
        "\n",
        "    data['Compensation Grade'] = data['Compensation Grade'].replace({'J060': '35', 'J070': '45', 'J080': '45', 'J090': '45',\n",
        "                                                                     'J100': '55', 'J110': '60', 'J120': '65', 'J130': '70',\n",
        "                                                                     'J140': '70'})\n",
        "    data['Compensation Grade'] = pd.to_numeric(data['Compensation Grade'], errors='coerce')\n",
        "    data = data[data['Compensation Grade'] >= 60]\n",
        "\n",
        "    processed_data = data.copy()\n",
        "    processed_data = processed_data.drop(['Retention', 'Areas Of Opportunity', 'Strengths', 'Potential Assessment Notes',\n",
        "                                          'Current Year -1 Review Rating', 'Current Year -2 Review Rating', 'Hire Date',\n",
        "                                          'Company', 'Location', 'Location Address - Country', 'Job Title',\n",
        "                                          'Compensation Grade', 'Segment', 'Cost Center', 'Functional Area Code',\n",
        "                                          'Functional Area Name', 'Functional Department', 'Reporting Unit',\n",
        "                                          'Current Year -1 Completed Review', 'Employee ID', 'Manager - Level 01',\n",
        "                                          'Manager Level 01', 'Manager Level 02', 'Manager Level 03', 'Manager Level 04',\n",
        "                                          'Manager Level 05', 'Manager Level 06', 'Manager Level 07', 'Manager Level 08',\n",
        "                                          'Manager Level 09', 'Supervisory Organization', 'Last Updated',\n",
        "                                          'Current Year -2 Completed Review'], axis=1)\n",
        "    processed_data = processed_data.dropna()\n",
        "\n",
        "    stop_words.update(['company'])\n",
        "    processed_data[column_name] = processed_data[column_name].apply(lambda x: sw_removal(x, stop_words))\n",
        "\n",
        "    vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))\n",
        "    column_values = processed_data[column_name].tolist()\n",
        "    X = vectorizer.fit_transform(column_values)\n",
        "\n",
        "    return processed_data, X, vectorizer\n",
        "\n",
        "def predict_categories(X, category_descriptions):\n",
        "    Y = vectorizer.transform(category_descriptions)\n",
        "    similarity_scores = cosine_similarity(X, Y)\n",
        "    predicted_categories = [list(categories.keys())[score.argmax()] for score in similarity_scores]\n",
        "    return predicted_categories\n",
        "\n",
        "\n",
        "input_file_path = 'Ryans_Report.csv'\n",
        "output_file_path = r'\\\\File Path'\n",
        "\n",
        "\n",
        "stop_words = set(['i', 'it', 'is', \"it's\", 'its', 'itself', 'which', 'a', 'an', 'the', 'and', 'but',\n",
        "                  'if', 'or', 'because', 'as', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'into',\n",
        "                  'to', 'from', 'in', 'out', 'then', 'there', 'so', 'me', 'my', 'we', 'has', 'have', 'had',\n",
        "                  'is', 'are', 'was', 'were', 'of', 'was', 'were', 'be', 'been', 'being'])\n",
        "\n",
        "data = pd.read_csv(input_file_path)\n",
        "\n",
        "strengths_data, X_strengths, vectorizer = process_data(data, 'Strengths', stop_words)\n",
        "areas_data, X_areas, _ = process_data(data, 'Areas Of Opportunity', stop_words)\n",
        "\n",
        "strengths_predicted_categories = predict_categories(X_strengths, category_descriptions)\n",
        "areas_predicted_categories = predict_categories(X_areas, category_descriptions)\n",
        "\n",
        "strengths_data['Strengths Predicted Category'] = strengths_predicted_categories\n",
        "areas_data['Areas Of Opportunity Predicted Category'] = areas_predicted_categories\n",
        "\n",
        "merged_data = strengths_data.merge(areas_data[['Worker', 'Areas Of Opportunity Predicted Category']], on='Worker', how='left')\n",
        "\n",
        "final_merged_data = data.merge(merged_data[['Worker', 'Strengths Predicted Category', 'Areas Of Opportunity Predicted Category']], on='Worker', how='left')\n",
        "\n",
        "final_merged_data.to_excel(output_file_path, index=False, encoding='utf-8')\n",
        "\n",
        "print(\"DataFrame exported to Excel with UTF-8 encoding successfully.\")\n"
      ]
    }
  ]
}